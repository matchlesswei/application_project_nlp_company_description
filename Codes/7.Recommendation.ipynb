{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pickle\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sn\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the preprocessed data from the data_directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install testfixtures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"Generated_Files/data_after_preprocessing.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We devide the data into 3 groups:\n",
    "* Group 1: full data\n",
    "* Group 2: data with four large categories which have more than 1000 companies each\n",
    "* Group 3: seven categories of data, number of companies in each category is same but small\n",
    "\n",
    "### In the function selectGroup, giving 1, 2 or 3 as input parameter to selet the relevant data for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from directory, then select the group \n",
    "# of data we want to process.\n",
    "def selectGroup(directory, group_nr):\n",
    "    data = pd.read_csv(directory, sep='\\t')\n",
    "    if group_nr == 1:\n",
    "        return data\n",
    "    if group_nr == 2:\n",
    "        df_healthcare_group=data[data['Category'] == 'HEALTHCARE GROUP'].sample(n=1041,replace=False)\n",
    "        df_business_financial_services=data[data['Category'] == 'BUSINESS & FINANCIAL SERVICES'].sample(n=1041,replace=False)\n",
    "        df_consumer_service_group=data[data['Category'] == 'CONSUMER SERVICES GROUP'].sample(n=1041,replace=False)\n",
    "        df_information_technology_group=data[data['Category'] == 'INFORMATION TECHNOLOGY GROUP'].sample(n=1041,replace=False)\n",
    "        df_clean = pd.concat([df_healthcare_group, df_business_financial_services,df_consumer_service_group,df_information_technology_group])\n",
    "        return df_clean.sample(frac=1)\n",
    "    if group_nr == 3:\n",
    "        df_healthcare_group=data[data['Category'] == 'HEALTHCARE GROUP'].sample(n=219,replace=False)\n",
    "        df_business_financial_services=data[data['Category'] == 'BUSINESS & FINANCIAL SERVICES'].sample(n=219,replace=False)\n",
    "        df_consumer_service_group=data[data['Category'] == 'CONSUMER SERVICES GROUP'].sample(n=219,replace=False)\n",
    "        df_information_technology_group=data[data['Category'] == 'INFORMATION TECHNOLOGY GROUP'].sample(n=219,replace=False)\n",
    "        df_industry_goods=data[data['Category'] == 'INDUSTRIAL GOODS & MATERIALS GROUP'].sample(n=219,replace=False)\n",
    "        df_consumer_goods=data[data['Category'] == 'CONSUMER GOODS GROUP'].sample(n=219,replace=False)\n",
    "        df_energy=data[data['Category'] == 'ENERGY & UTILITIES GROUP'].sample(n=219,replace=False)\n",
    "        df_clean = pd.concat([df_healthcare_group, df_business_financial_services,df_consumer_service_group,df_information_technology_group,df_industry_goods,df_consumer_goods,df_energy])\n",
    "        return df_clean.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and Split the data\n",
    "data = selectGroup(data_directory, 1)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "Web=train['Web'].append(test['Web'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data and generate vectors through different methods - Doc2Vec, TF-IDF, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate vectors from Doc2ec\n",
    "#Load the doc2vec model and Generate tagged documents\n",
    "filename = 'Generated_Files/doc2vec_model.sav'\n",
    "new_model = pickle.load(open(filename, 'rb'))\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean']), tags=[r.Category]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean']), tags=[r.Category]), axis=1)\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer vector done for train data\n"
     ]
    }
   ],
   "source": [
    "# Infer vectors from doc2vec model\n",
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors\n",
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "print(\"Infer vector done for train data\")\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)\n",
    "\n",
    "X_doc2vec = X_train + X_test\n",
    "y_doc2vec = y_train + y_test\n",
    "X_doc2vec = csr_matrix(pd.DataFrame(list(X_doc2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate vectors from TF-IDF\n",
    "feature_extraction = TfidfVectorizer( min_df = 5,\n",
    "    max_df = 0.95,\n",
    "    max_features = 8000,\n",
    "    #ngram_range=(1, 2),\n",
    "    stop_words = 'english')\n",
    "X_tfidf = feature_extraction.fit_transform(data['clean'].values)\n",
    "y_tfidf = data['Category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate vectors from Word2Vec\n",
    "def tf_idf_func(df_document, max_features):\n",
    "    feature_extraction = TfidfVectorizer(max_features = max_features, stop_words = 'english')\n",
    "    score_matrix = feature_extraction.fit_transform(df_document.values)\n",
    "    return score_matrix, feature_extraction\n",
    "\n",
    "def get_top_keywords_with_frequence(Top_N, score_matrix, df_data, feature_extraction):\n",
    "    df = pd.DataFrame(score_matrix.todense())\n",
    "    df['Category'] = df_data['Category']\n",
    "    dfg = df.groupby(['Category']).mean()\n",
    "\n",
    "    labels = feature_extraction.get_feature_names()\n",
    "\n",
    "    categories = df_data['Category'].unique()\n",
    "    col_names = ['Category', 'Top_N', 'Score']\n",
    "    df_top = pd.DataFrame(columns = col_names)\n",
    "\n",
    "    Dict = {}\n",
    "\n",
    "    for i,r in dfg.iterrows():\n",
    "        category = i \n",
    "        top_series = np.argsort(r)[-Top_N:]\n",
    "        label_series = top_series.apply(lambda x: labels[x])\n",
    "        top_scores = np.sort(r)[-Top_N:]\n",
    "        df_each = pd.DataFrame({'Category':category,'Top_N':label_series,'Score':top_scores})\n",
    "        df_top = df_top.append(df_each, ignore_index = True)\n",
    "        for key in label_series:\n",
    "            if key in Dict:\n",
    "                Dict[key] = Dict[key]+1\n",
    "            else:\n",
    "                Dict[key] = 1\n",
    "    \n",
    "    df_reshape = df_top.pivot(index='Top_N', columns='Category')\n",
    "    sortedDict = sorted(Dict.items(), key=lambda x: x[1])\n",
    "    \n",
    "    return sortedDict, df_reshape\n",
    "\n",
    "def get_word_occurence_stopwordslist(max_occurence, dict_list):\n",
    "    word = []\n",
    "    occurence = []\n",
    "    frequent_stopwords = []\n",
    "    for key, value in dict_list:\n",
    "        word.append(key)\n",
    "        occurence.append(value)\n",
    "        if value > max_occurence:\n",
    "            frequent_stopwords.append(key)\n",
    "    return word, occurence, frequent_stopwords\n",
    "    \n",
    "def remove_frequent_stopwords(sentences, frequent_stopwords):\n",
    "    splitted_string = sentences.split()\n",
    "    remove_stopwords = [w for w in splitted_string if not w in frequent_stopwords]\n",
    "    return ' '.join(remove_stopwords)\n",
    "\n",
    "def remove_frequent_stopwords_and_get_updated_tfidfscore(data, feature_extraction, top_n, frequent_stopwords):\n",
    "    df_update = data['clean'].apply(lambda x: remove_frequent_stopwords(x, frequent_stopwords))\n",
    "    score_matrix_update = feature_extraction.fit_transform(df_update.values)\n",
    "    return score_matrix_update\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "score_matrix, feature_extraction = tf_idf_func(data['clean'], 8000)\n",
    "sortedDict, df_reshape = get_top_keywords_with_frequence(50, score_matrix, data, feature_extraction)\n",
    "word, occurence, frequent_stopwords = get_word_occurence_stopwordslist(1, sortedDict)\n",
    "score_matrix_update = remove_frequent_stopwords_and_get_updated_tfidfscore(data, feature_extraction, 10, frequent_stopwords)\n",
    "score_value = score_matrix_update.todense()\n",
    "website_word_count=np.asarray(np.count_nonzero(score_value, axis=1)).reshape(-1)\n",
    "\n",
    "df_score=pd.DataFrame(score_value)\n",
    "df_score.columns=feature_extraction.get_feature_names()\n",
    "df_score['Keep']=website_word_count>200\n",
    "category_temp = data['Category'].reset_index(drop=True)\n",
    "df_score['Category']=category_temp\n",
    "df_score['Web'] = data['Web'].reset_index(drop=True)\n",
    "df_score_valid = df_score[df_score['Keep']]\n",
    "df_final = df_score_valid[df_score_valid.columns.difference(['Web','Keep', 'Category'])]\n",
    "top_n = 100\n",
    "df_top_N = pd.DataFrame({n: df_final.T[col].nlargest(top_n).index.tolist() \n",
    "                  for n, col in enumerate(df_final.T)}).T\n",
    "\n",
    "df_category = df_score_valid['Category'].reset_index(drop=True)\n",
    "df_web = df_score_valid['Web'].reset_index(drop=True)\n",
    "df_top_N['Category'] = df_category\n",
    "df_top_N['Web'] = df_web\n",
    "\n",
    "def get_vector_from_df(df):\n",
    "    x_df = df[df.columns.difference(['Category'])]\n",
    "    x_word = x_df.to_numpy()\n",
    "    x = np.zeros([len(x_word), 300])\n",
    "    for i in range(len(x_word)):\n",
    "        initial_vector = np.zeros(300)\n",
    "        unseen_word = 0\n",
    "        for j in range(top_n):\n",
    "            try:\n",
    "                initial_vector = initial_vector + model.wv[x_word[i,j]]\n",
    "            except KeyError as e:\n",
    "                unseen_word = unseen_word + 1\n",
    "        final_vector = initial_vector/(top_n-unseen_word)\n",
    "        if np.isnan(np.sum(final_vector)):\n",
    "            print(i)\n",
    "            final_vector = np.zeros([1,300])\n",
    "        x[i] = final_vector\n",
    "    return x\n",
    "\n",
    "X_word2vec = get_vector_from_df(df_top_N)\n",
    "X_word2vec=np.nan_to_num(X_word2vec)\n",
    "y_word2vec = df_top_N['Category'].to_numpy()\n",
    "X_word2vec = csr_matrix(pd.DataFrame(list(X_word2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the corresponding website names\n",
    "Website_tfidf = data['Web'].values\n",
    "Website_word2vec = df_top_N['Web'].values\n",
    "Website_doc2vec = Web.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the recommend function based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(Input_Company, top_k, X_vector, y_vector, Website): \n",
    "    Input=X_vector[np.where(Website==Input_Company)[0][0]]\n",
    "    Similarity = np.zeros((X_vector.shape)[0])\n",
    "    for index, vector in enumerate(X_vector):\n",
    "        Similarity[index]=np.round(cosine_similarity(Input, vector), 2)\n",
    "    output=np.flipud(Similarity.argsort()[(-1-top_k):-1])\n",
    "    for i in output:\n",
    "        print(\"Website: \", Website[i], \" Category:\", y_vector[i], \" Similarity:\", Similarity[i],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Inputs such as the number of companies to recommend and the input company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Web</th>\n",
       "      <th>Category</th>\n",
       "      <th>IndustrySegment</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>5519</td>\n",
       "      <td>www.sbamerica.com</td>\n",
       "      <td>CONSUMER GOODS GROUP</td>\n",
       "      <td>Food and Beverage</td>\n",
       "      <td>Specialty Brands of America, Inc. [\"SBA\"] was ...</td>\n",
       "      <td>specialty brands america inc sba established b...</td>\n",
       "      <td>5886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                Web              Category    IndustrySegment  \\\n",
       "5170        5519  www.sbamerica.com  CONSUMER GOODS GROUP  Food and Beverage   \n",
       "\n",
       "                                                content  \\\n",
       "5170  Specialty Brands of America, Inc. [\"SBA\"] was ...   \n",
       "\n",
       "                                                  clean  size  \n",
       "5170  specialty brands america inc sba established b...  5886  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input company and the number of companies to recommend\n",
    "top_k = 5\n",
    "Input_Company = \"www.sbamerica.com\"\n",
    "data[data['Web']==Input_Company]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend companies based on different methods - Doc2Vec, TF-IDF, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website:  www.celiant.com  Category: INFORMATION TECHNOLOGY GROUP  Similarity: 0.93\n",
      "Website:  www.packetmotion.com  Category: INFORMATION TECHNOLOGY GROUP  Similarity: 0.93\n",
      "Website:  www.huskietools.com  Category: INDUSTRIAL GOODS & MATERIALS GROUP  Similarity: 0.93\n",
      "Website:  www.multiwavenetworks.com  Category: INFORMATION TECHNOLOGY GROUP  Similarity: 0.93\n",
      "Website:  www.appbackr.com  Category: BUSINESS & FINANCIAL SERVICES  Similarity: 0.93\n"
     ]
    }
   ],
   "source": [
    "#Doc2Vec Similarity\n",
    "recommend(Input_Company, top_k, X_doc2vec, y_doc2vec, Website_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website:  www.republicind.com  Category: CONSUMER GOODS GROUP  Similarity: 0.35\n",
      "Website:  www.sirkensingtons.com  Category: CONSUMER GOODS GROUP  Similarity: 0.23\n",
      "Website:  www.wholesomesweeteners.com  Category: CONSUMER GOODS GROUP  Similarity: 0.21\n",
      "Website:  www.bakewisebrands.com  Category: CONSUMER GOODS GROUP  Similarity: 0.15\n",
      "Website:  www.pure360.com  Category: BUSINESS & FINANCIAL SERVICES  Similarity: 0.15\n"
     ]
    }
   ],
   "source": [
    "#TFIDF Similarity\n",
    "recommend(Input_Company, top_k, X_tfidf, y_tfidf, Website_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website:  www.thanasi.com  Category: CONSUMER SERVICES GROUP  Similarity: 0.92\n",
      "Website:  www.mainstreetgourmet.com  Category: CONSUMER GOODS GROUP  Similarity: 0.91\n",
      "Website:  www.bellisiofoods.com  Category: CONSUMER GOODS GROUP  Similarity: 0.91\n",
      "Website:  www.caesarspasta.com  Category: CONSUMER GOODS GROUP  Similarity: 0.9\n",
      "Website:  www.jjsbakery.net  Category: CONSUMER GOODS GROUP  Similarity: 0.89\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec Similarity\n",
    "recommend(Input_Company, top_k, X_word2vec, y_word2vec, Website_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data processed by word2vec for GUI usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word2vec results to files for GUI\n",
    "scipy.sparse.save_npz('Generated_Files/X_word2vec.npz', X_word2vec)\n",
    "np.save('Generated_Files/y_word2vec.npy', y_word2vec)\n",
    "np.save('Generated_Files/Website_word2vec.npy', Website_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
